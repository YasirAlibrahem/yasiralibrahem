---
title: Fundamental Concepts in GenAI
date: Mar 2nd, 2024
cover: cover02.png
excerpt: "The rapid pace at which new developments in the Generative AI (GenAI) field were announced, weekly if not daily, leading to increased excitement and ambiguity. Today, we seem to have reached a level of comfort and maturity as we understood the capabilities of the technology, its current limitation, and future direction. Let's review foundational concepts to in Machine Learning, all the way to GPT."
---
OpenAI released ChatGPT back on Nov 30, 2022, 17 months ago. It marked a pivotal point in history ushering a new truly transformative era of technology. This technological evolution has impact all aspects of life, nudging us to revise how we interact with the world, our day-to-day routines, and rethink our future plans. The rapid pace at which new developments in the Generative AI (GenAI) field were announced, weekly if not daily, leading to increased excitement and ambiguity. Today, we seem to have reached a level of comfort and maturity as we understood the capabilities of the technology, its current limitation, and future direction. Let's review foundational concepts to in Machine Learning, all the way to GPT.

### Supervised, Unsupervised Learning

Machine Learning (ML), a pivotal subfield of Artificial Intelligence (AI), has evolved significantly over nearly a century. It encompasses key methodologies such as supervised learning, which trains models on labeled data sets, and unsupervised learning, which leverages algorithms to uncover patterns in unlabeled data. Supervised learning is utilized in applications like classification and regression, while unsupervised learning is often applied in clustering. The *Learning Curve (Training Curve)* is a measure to evaluate a model's performance and learning progress over time.

### Reinforcement Learning (RL)

An approach of ML where an *agent* is learning in an interactive *environment* by trial and error from its own *actions*. The feedback from rewards or penalties of each iteration helps the agent increase its accuracy over time. Rewards in this context refers to a positive evaluation of the actions taken by the agent. Often used in goal-driven systems. 

### Transfer Learning

An ML approach leveraging a pre-trained model and fine-tuning to make predictions for a new problem as an alternative to training a model from scratch. This saves computational cost, time, and can require a smaller training set for the new problem as the model put to use has been already trained to an extent. Transfer learning has been applied in Computer Vision and Natural Language Processing (NLP). 

### Deep Learning

A machine learning branch using neural networks with multiple hidden layers in the hundreds or thousands. Deep learning popularity expanded with the adoption of backpropagation back in the 1980’s, model pretraining, Big Data, and increased performance of GPU’s to match the heavy demand to train large models. 

### Artificial Neural Networks (ANNs)

Often referred to as Neural Networks —Neural Nets for short— are a branch of ML algorithms that resembles the design of human brains neurons in the form of a layered group of connected nodes, with *weights* and *biases* to produce an output. The algorithm learns the weights and biases through iterations on the training set. The *Perceptron* was the first ANN, dating back to the 1940s with a single layer of neurons to predict an output based on input data. 

There are many types of NN’s such classification, clustering, and regression. Recurrent Neural Networks (RNN’s) and Convolutional Neural Networks (CNN’s) are variants that have gained more popularity recently. CNN’s are good at dealing with images and videos while RNN’s are good at time-series data such as Natural Language Processing (NLP) and video. A type of RNN called Long Short-Term Memory (LTSM) has built-in loops and short-term memory that lasts thousands of timestamps, thus the name. LTSM were gaining traction till the Transformer architecture came into play with the same applications.

### Generative Adversarial Network (GAN)

A deep learning architecture comprising two NN’s, used for content generation. The first —the *generator*— generating a representation of the output while the second —the *d*i*scriminator*— determines how suitable that output is to the desired outcome. The two NN’s are trained simultaneously by an adversarial process. An example is a GAN that generates images of mountains where the *generator’s* output images are reviewed by the *discriminator*. 

### Model

At its most basic definition, a model is a live system that processes input data through an algorithm to learn and discern patterns and generate predictions. A model’s output accuracy and precision is evaluated in an F1-score (range 0-1). In other words, a model is a learned algorithm from a *learning dataset* that can be deployed to make predictions on unseen data. Examples of models are AlexNet, ResNet, AlphaGo, Stable Diffusion, and ChatGPT. 

### Overfitting and Underfitting

A model that’s tightly tied to the training dataset that it’s less capable of generalizing is said to be overfitting. In other words, the model predictions accuracy is high on training and evaluation data sets but degrade when unseen data is presented. That model has captured both noise and outliers from the training data, rendering it ineffective. The opposite, underfitting, occurs when the model learning lacked in capturing the underlying structure of the training data and fails to generate accurate predictions. 

### Fine-tuning

As training a model is computationally complex and costly operation that can go into the millions of dollars, techniques were developed to better fit a model to an application with less hefty investment. Fine-tuning involves adjusting a pre-trained model with a smaller dataset. Often, the earlier layers in a DNN are “frozen” so the training adjustment influences deeper layers. Some of the current efficient techniques include Low Rank Adaption (LoRA), Quantization LoRA (Q-LoRA), and Progressive Error Feedback Tuning (PEFT). These techniques are not mutually-exclusive and can be combined to yield higher accuracy.

### Ensemble Models

An approach to ML to train multiple models on the same data then aggregating the outputs to produce a final prediction of higher accuracy than that of each independent model.

### Classification

Grouping data into *classes* or *categories*. Applications include object recognition, optical character recognition, and sentiment analysis. Types of *Classifiers* include binary/binomial and multiclass/multinomial. When graphed, categories are separated by *Decision Lines*. Some common algorithms are *K-nearest Neighbors, Support Vector Machines (SVM), Random Forest,* and *Decision Trees. SVMs* are used both for linear and nonlinear classification and work by finding the hyperplane that divides datasets into classes. Decision Trees leverage a tree-like model of decisions that’s easier to interpret, used for both categorical and numerical features. Random Forests are an ensemble method that combines multitudes of decision trees. 

### Clustering

An technique of unsupervised learning used to group similar objects. Clustering differs from classification in that it operates on unlabeled data. Common algorithms includes *K-means, Hierarchical Clustering, and DBSCAN.*

### Regression

In ML, regression algorithms are used to determine continuous numerical values based on input features. For example, predicting credit scores, prices, stock valuations, ages, and temperatures. Common algorithms include *Linear Regression, Polynomial Regression, Support Vector Regression (SVR),* and *Decision Tree Regression.* Regression models are widely used in finance, healthcare, and real estate.

### Parameters and Hyperparameters

Inputs to an ML model. Parameters are learned by the model from the dataset during training, such as the weights and biases in a neural network. They model optimizes values on its own. Hyperparameters on the other hand are constants set prior to the training process. Hyperparameters are used to control the learning process and influence model performance, such as *K* in K-Nearest Neighbors, the *learning rate* in gradient descent, the number of trees in a random forest, and the depth of the trees. Techniques to optimize hyperparameters include *Grid Search, Random Search,* and *Bayesian Optimization*. ML Engineers and Data Scientists fine-tune models by adjusting hyperparameters.

### Features and Feature Engineering

*Feature* are inputs variables to a model. Features represent the dataset characteristics or attributes. One simplistic one to think of features is the columns in a large spreadsheet. *Features* and *Parameters* are different and should not be used interchangeably. Feature engineering is the manipulation or transformation of features in an attempt to increase model accuracy. For example, feature creation, normalization, converting categorical features to numerical (one hot encoding), feature reduction (PCA, feature selection), and feature extraction. It’s worth noting that in deep learning, especially RNNs and CNNs, algorithms are able to automatically identify and extract features of most importance to learning. 

### Weights and Biases

Weights are parameters that represent the connection strength between two consecutive neurons in a DNN. Weights determine the influence of one neuron on another, and therefore influence whether the deeper layer neuron will activate. Biases are learnable parameters used alongside the weighted sum of inputs to adjust the output of a neuron. Each neuron maintains its own bias. Bias is a part of the mechanism the NN uses to produce its predictions, adjusting the linearity of the model’s decision boundary. Combined with weights, bias helps the NN to fit the training data more accurately, allowing it to model complex patterns.  

### Activation Function, Cost Function, and Loss Function

*Activation functions* calculate the weighted sum of inputs, add bias, then decide whether a neuron should activate or not. In essence, activation functions transform input an input signal into an output signal. Common functions include *Rectified Linear Unit (ReLU), Sigmoid*, and *Softmax*. A *Cost Function,* used interchangeably with *Loss Function* is an aggregate of all errors (losses) in training. Models aim to minimize the cost function. One example method is *Mean Squared Error (MSE)* that’s used often in regression and calculates the square of the difference between the predicted and actual values. Another examples is *Mean Absolute Error (MAE)* which measures the absolute difference between predicated and actual values. 

### Epoch and Learning Rate

An *Epoch* is one pass through the entire training dataset while training a model. During the pass, the model updates its weights and biases in response to the loss function. The number of epochs is set as a hyperparameters and can go into the thousands. It’s worth noting that more isn’t always better as it lead to overfitting. *Learning Rate* is a hyperparameter dictating the rate at which model change in response to the cost function with each epoch. 

### Gradient Descent

A deep learning optimization algorithm used to minimize the cost function by iteratively adjust the model’s parameters (weights & biases) in the opposite direction of a gradient of the cost function with respect to the parameters, based on a learning rate. Stochastic Gradient Descent (SGD) or mini-batch gradient descent are often used where the weights are updated in a single sample or a small batch of samples leading to faster convergence. 

### Transformer Architecture, Generative Pre-trained Transformer (GPT)

Let’s put it all together to where GPT is. A *Transformer* is a deep learning model architecture that processes sequential data and is based on *attention* mechanisms solely without the use of RNNs and CNNs. The introduction of this architecture dates back to 2017 in the paper *Attention Is All You Need*. It’s improved efficiency and scalability in model training and inference. *Self-Attention Mechanism* enables the model to weigh the significance of different parts of the input data independently, thus, focusing the model on parts of the input sequence as required. Transformer architecture is the foundation of OpenAI’s GPT models. Those models are pre-trained on massive datasets, in the billions, and are optimized for large-scale language modeling. 